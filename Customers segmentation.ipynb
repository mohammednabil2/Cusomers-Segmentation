{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport datetime\nfrom datetime import date\nimport matplotlib\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nfrom sklearn.preprocessing import StandardScaler, normalize\nfrom sklearn import metrics\nfrom sklearn.mixture import GaussianMixture\nfrom mlxtend.frequent_patterns import apriori\nfrom mlxtend.frequent_patterns import association_rules\nimport warnings\nwarnings.filterwarnings('ignore')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=pd.read_csv('/kaggle/input/customer-personality-analysis/marketing_campaign.csv',sep=\"\\t\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install dataprep\n","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from dataprep.eda import plot, plot_correlation, create_report, plot_missing\nplot(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**we have missing values in the Income table and as this is a clustring not a predicting problem , we can't really guess what the income is , so we will drop the missing values**","metadata":{}},{"cell_type":"code","source":"data = data.dropna()\nprint(\"The total number of data-points after removing the rows with missing values are:\", len(data))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now regarding the Dt_customer , we don't know what date should we use in order to extract number of days for each customer \nso i will just use the most recent record in our data**","metadata":{}},{"cell_type":"code","source":"data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['Dt_Customer'] = pd.to_datetime(data['Dt_Customer'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_date = data['Dt_Customer'].max()\ndata['_num_days'] = (max_date - data['Dt_Customer']).dt.days","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Based on the recent dates , I'm going to assume that we live in 2015 .. \nalso we notice a lot of values in the categorical features \nhere comes the feature engineering part**","metadata":{}},{"cell_type":"code","source":"#Feature Engineering\n#Age of customer today \ndata[\"Age\"] = 2015-data[\"Year_Birth\"]\n\n#Total spendings on various items\ndata[\"Spent\"] = data[\"MntWines\"]+ data[\"MntFruits\"]+ data[\"MntMeatProducts\"]+ data[\"MntFishProducts\"]+ data[\"MntSweetProducts\"]+ data[\"MntGoldProds\"]\n\n#Deriving living situation by marital status\"Alone\"\ndata[\"Living_With\"]=data[\"Marital_Status\"].replace({\"Married\":\"Partner\", \"Together\":\"Partner\", \"Absurd\":\"Alone\", \"Widow\":\"Alone\", \"YOLO\":\"Alone\", \"Divorced\":\"Alone\", \"Single\":\"Alone\",})\n\n#Feature indicating total children living in the household\ndata[\"Children\"]=data[\"Kidhome\"]+data[\"Teenhome\"]\n\n#Feature for total members in the householde\ndata[\"Family_Size\"] = data[\"Living_With\"].replace({\"Alone\": 1, \"Partner\":2})+ data[\"Children\"]\n\n#Feature pertaining parenthood\ndata[\"Is_Parent\"] = np.where(data.Children> 0, 1, 0)\n\n#Segmenting education levels in three groups\ndata[\"Education\"]=data[\"Education\"].replace({\"Basic\":\"Undergraduate\",\"2n Cycle\":\"Undergraduate\", \"Graduation\":\"Graduate\", \"Master\":\"Postgraduate\", \"PhD\":\"Postgraduate\"})\n\n#For clarity\ndata=data.rename(columns={\"MntWines\": \"Wines\",\"MntFruits\":\"Fruits\",\"MntMeatProducts\":\"Meat\",\"MntFishProducts\":\"Fish\",\"MntSweetProducts\":\"Sweets\",\"MntGoldProds\":\"Gold\"})\n\n#Dropping some of the redundant features\nto_drop = [\"Marital_Status\", \"Dt_Customer\", \"Z_CostContact\", \"Z_Revenue\", \"Year_Birth\", \"ID\"]\ndata = data.drop(to_drop, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import colors\nfrom matplotlib.colors import ListedColormap\nsns.set(rc={\"axes.facecolor\":\"#FFF9ED\",\"figure.facecolor\":\"#FFF9ED\"})\npallet = [\"#682F2F\", \"#9E726F\", \"#D6B2B1\", \"#B9C0C9\", \"#9F8A78\", \"#F3AB60\"]\ncmap = colors.ListedColormap([\"#682F2F\", \"#9E726F\", \"#D6B2B1\", \"#B9C0C9\", \"#9F8A78\", \"#F3AB60\"])\n#Plotting following features\nTo_Plot = [ \"Income\", \"Recency\", \"_num_days\", \"Age\", \"Spent\", \"Is_Parent\"]\nprint(\"Reletive Plot Of Some Selected Features: A Data Subset\")\nplt.figure()\nsns.pairplot(data[To_Plot], hue= \"Is_Parent\",palette= ([\"#682F2F\",\"#F3AB60\"]))\n#Taking hue \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**conclusion : We have a lot of work to do with outliers**","metadata":{}},{"cell_type":"code","source":"q1_income = np.percentile(data['Income'], 25)\nq3_income = np.percentile(data['Income'], 75)\n\nq1_age = np.percentile(data['Age'], 25)\nq3_age = np.percentile(data['Age'], 75)\n\niqr_income = q3_income - q1_income\niqr_age = q3_age - q1_age\n\nthreshold = 1.5\n\noutliers_income = data[(data['Income'] < q1_income - threshold * iqr_income) | (data['Income'] > q3_income + threshold * iqr_income)]\noutliers_age = data[(data['Age'] < q1_age - threshold * iqr_age) | (data['Age'] > q3_age + threshold * iqr_age)]\n\nprint(\"Outliers in 'Income':\")\nprint(outliers_income)\n\nprint(\"\\nOutliers in 'Age':\")\nprint(outliers_age)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop the maximum value in 'Income'\nmax_income = data['Income'].max()\ndata = data[data['Income'] != max_income]\n\n# Remove outliers in 'Age'\nq1_age = np.percentile(data['Age'], 25)\nq3_age = np.percentile(data['Age'], 75)\niqr_age = q3_age - q1_age\nthreshold = 1.5\n\nlower_bound = q1_age - threshold * iqr_age\nupper_bound = q3_age + threshold * iqr_age\n\ndata = data[(data['Age'] >= lower_bound) & (data['Age'] <= upper_bound)]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. Stars: Old customers with high income and high spending nature.\n2. Neet Attention: New customers with below-average income and low spending nature. \n3. High Potential: New customers with high income and high spending nature.\n4. Leaky Bucket: Old customers with below-average income and a low spending nature.\n","metadata":{}},{"cell_type":"code","source":"scaler=StandardScaler()\ndataset_temp=data[['Income','_num_days','Spent']]\nX_std=scaler.fit_transform(dataset_temp)\nX = normalize(X_std,norm='l2')\n\ngmm=GaussianMixture(n_components=4, covariance_type='spherical',max_iter=2000, random_state=5).fit(X)\nlabels = gmm.predict(X)\ndataset_temp['Cluster'] = labels\ndataset_temp=dataset_temp.replace({0:'Stars',1:'Need attention',2:'High potential',3:'Leaky bucket'})\ndata = data.merge(dataset_temp.Cluster, left_index=True, right_index=True)\n\npd.options.display.float_format = \"{:.0f}\".format\nsummary=data[['Income','Spent','_num_days','Cluster']]\nsummary.set_index(\"Cluster\", inplace = True)\nsummary=summary.groupby('Cluster').describe().transpose()\nsummary.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\n# Select the features for clustering\nfeatures = ['Income', '_num_days', 'Spent']\n\n# Create a new dataframe with the selected features\ndata_cluster = data[features].copy()\n\n# Scale the data using StandardScaler\nscaler = StandardScaler()\ndata_scaled = scaler.fit_transform(data_cluster)\n\n# Apply k-means clustering\nk = 4  # Number of clusters\nkmeans = KMeans(n_clusters=k, random_state=5)\nlabels = kmeans.fit_predict(data_scaled)\n\n# Map cluster labels to names\ncluster_names = {0: 'Stars', 1: 'Need attention', 2: 'High potential', 3: 'Leaky bucket'}\ncluster_labels = [cluster_names[label] for label in labels]\n\n# Assign cluster labels to the original dataset\ndata['Cluster'] = cluster_labels\n\n# Create a summary dataframe\nsummary = data.groupby('Cluster')[features].describe().transpose()\n\nsummary\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Well , K means makes more sense to me !**","metadata":{}},{"cell_type":"code","source":"PLOT = go.Figure()\nfor C in list(data.Cluster.unique()):\n    \n\n    PLOT.add_trace(go.Scatter3d(x = data[data.Cluster == C]['Income'],\n                                y = data[data.Cluster == C]['_num_days'],\n                                z = data[data.Cluster == C]['Spent'],                        \n                                mode = 'markers',marker_size = 6, marker_line_width = 1,\n                                name = str(C)))\nPLOT.update_traces(hovertemplate='Income: %{x} <br>Seniority: %{y} <br>Spending: %{z}')\n\n    \nPLOT.update_layout(width = 800, height = 800, autosize = True, showlegend = True,\n                   scene = dict(xaxis=dict(title = 'Income', titlefont_color = 'black'),\n                                yaxis=dict(title = '_num_days', titlefont_color = 'black'),\n                                zaxis=dict(title = 'Spent', titlefont_color = 'black')),\n                   font = dict(family = \"Gilroy\", color  = 'black', size = 12))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pal = [\"#682F2F\",\"#B9C0C9\", \"#9F8A78\",\"#F3AB60\"]\npl = sns.countplot(x=data[\"Cluster\"], palette= pal)\npl.set_title(\"Distribution Of The Clusters\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cut_labels_Age = ['Young', 'Adult', 'Mature', 'Senior']\ncut_bins = [0, 30, 45, 65, 120]\ndata['Age_group'] = pd.cut(data['Age'], bins=cut_bins, labels=cut_labels_Age)\n#Create Income segment\ncut_labels_Income = ['Low income', 'Low to medium income', 'Medium to high income', 'High income']\ndata['Income_group'] = pd.qcut(data['Income'], q=4, labels=cut_labels_Income)\n#Create Seniority segment\ncut_labels_Seniority = ['New customers', 'Discovering customers', 'Experienced customers', 'Old customers']\ndata['Seniority_group'] = pd.qcut(data['_num_days'], q=4, labels=cut_labels_Seniority)\ndata=data.drop(columns=['Age','Income','_num_days'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cut_labels = ['Low consumer', 'Frequent consumer', 'Biggest consumer']\ndata['Wines_segment'] = pd.qcut(data['Wines'][data['Wines']>0],q=[0, .25, .75, 1], labels=cut_labels).astype(\"object\")\ndata['Fruits_segment'] = pd.qcut(data['Fruits'][data['Fruits']>0],q=[0, .25, .75, 1], labels=cut_labels).astype(\"object\")\ndata['Meat_segment'] = pd.qcut(data['Meat'][data['Meat']>0],q=[0, .25, .75, 1], labels=cut_labels).astype(\"object\")\ndata['Fish_segment'] = pd.qcut(data['Fish'][data['Fish']>0],q=[0, .25, .75, 1], labels=cut_labels).astype(\"object\")\ndata['Sweets_segment'] = pd.qcut(data['Sweets'][data['Sweets']>0],q=[0, .25, .75, 1], labels=cut_labels).astype(\"object\")\ndata['Gold_segment'] = pd.qcut(data['Gold'][data['Gold']>0],q=[0, .25, .75, 1], labels=cut_labels).astype(\"object\")\ndata.replace(np.nan, \"Non consumer\",inplace=True)\ndata.drop(columns=['Spent','Wines','Fruits','Meat','Fish','Sweets','Gold'],inplace=True)\ndata = data.astype(object)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom mlxtend.frequent_patterns import apriori\nfrom mlxtend.frequent_patterns import association_rules\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.max_colwidth', 999)\npd.options.display.float_format = \"{:.3f}\".format\n\nassociation = data.copy()\ndf = pd.get_dummies(association)\nmin_support = 0.08\nmax_len = 10\nfrequent_items = apriori(df, use_colnames=True, min_support=min_support, max_len=max_len + 1)\nrules = association_rules(frequent_items, metric='lift', min_threshold=1)\n\nproduct = 'Wines'  \nsegment = 'Biggest consumer'  \ntarget = f\"{{'{product}_segment_{segment}'}}\"\nresults = rules[rules['consequents'].astype(str).str.contains(target, na=False)].sort_values(by='confidence', ascending=False)\nresults.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}